{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "def preprocessar_dados_wikipedia(url_pagina):\n",
        "  # Faça uma requisição HTTP para a página da Wikipedia\n",
        "  resposta = requests.get(url_pagina)\n",
        "\n",
        "  # Extraia o conteúdo HTML da resposta\n",
        "  html = resposta.text\n",
        "\n",
        "  # Crie um dataframe a partir do HTML\n",
        "  df = pd.read_html(html)[0]\n",
        "\n",
        "  # Selecione apenas os primeiros 50 artigos\n",
        "  df = df.head(50)\n",
        "\n",
        "  # Adicione uma coluna com os links dos artigos\n",
        "  df[\"Link\"] = url_pagina + df[\"Título\"].apply(lambda x: x.replace(\" \", \"_\"))\n",
        "\n",
        "  return df\n",
        "\n",
        "# Chame a função com a URL da página da Wikipedia\n",
        "url_pagina = \"https://pt.wikipedia.org/wiki/Lista_de_fil%C3%B3sofos\"\n",
        "df = preprocessar_dados_wikipedia(url_pagina)\n",
        "\n",
        "# Crie uma conexão com o servidor SQL\n",
        "engine = create_engine(\"postgresql://user:password@localhost:5432/database\")\n",
        "\n",
        "# Armazene o dataframe no banco de dados\n",
        "df.to_sql(\"artigos\", engine, if_exists=\"replace\")\n",
        "\n",
        "print(\"Dados armazenados com sucesso no banco de dados!\")"
      ],
      "metadata": {
        "id": "4sCyxYtYEw1K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}